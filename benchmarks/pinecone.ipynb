{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import Features, Value, Sequence\n",
    "\n",
    "NUM_PROC = os.cpu_count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is a modified version of https://github.com/serega/gaoya/blob/master/py-gaoya/examples/deduplication_scholarly_articles_gaoya.ipynb, which benchmarks the algorithm on \n",
    "the `pinecone/core-2020-05-10-deduplication` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f318857d2ea408586ac4d8769ba0e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the script\n",
    "# we don't need to save the intermediate dataset into a variable unless we are debugging it\n",
    "(\n",
    "    datasets.load_dataset(\"pinecone/core-2020-05-10-deduplication\", split=\"train\",cache_dir=\"./cache\", num_proc=NUM_PROC)\n",
    "        .map(lambda x: {\"text\": \" \".join((x[\"processed_title\"],x[\"processed_abstract\"])).lower()}, num_proc=NUM_PROC)\n",
    "        .save_to_disk(\"temp_inp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_from_disk(\"temp_inp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe2081669ca43e893e31d4e7636156f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# it seems that this code cannot be multithreaded due to some kind of race condition\n",
    "truth = ds.map(\n",
    "    lambda x, id: {\"core_id\": x[\"core_id\"], \"id\": id, \"duplicates\": x[\"labelled_duplicates\"]}, \n",
    "    remove_columns=ds.column_names, \n",
    "    with_indices=True,\n",
    "    num_proc=os.cpu_count(),\n",
    "    features=Features({\n",
    "        \"core_id\": Value(\"string\"),\n",
    "        \"id\": Value(\"int64\"),\n",
    "        \"duplicates\": Sequence(Value(\"string\")),\n",
    "    })\n",
    ")\n",
    "id2core_id = {x[\"id\"]: int(x[\"core_id\"]) for x in truth}\n",
    "labels = {int(x[\"core_id\"]): set(map(int, x[\"duplicates\"])) if x[\"duplicates\"] else set() for x in truth}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinHash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no character shingle tokenizer in the script, you can either modify the code or use an n-gram tokenizer. For simplicity, we use bigrams in this example. Other parameters are the same as the original script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "!python -m text_dedup.minhash --path ./temp_inp --local --column text --num_perm 200 --ngram 2 --threshold 0.5 --output temp --split train --debug --b 50 --r 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Correct': 92401,\n",
       " 'Incorrect': 7599,\n",
       " 'Accuracy': 0.924,\n",
       " 'Recall': 0.9646,\n",
       " 'Precision': 0.4434}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _recall(row):\n",
    "    labelled_dups = set(row['duplicates'])\n",
    "    LEN_LABELLED_DUPLICATES = len(labelled_dups)    \n",
    "    if LEN_LABELLED_DUPLICATES == 0:\n",
    "        return 1\n",
    "    dups = set(row['predictions'])\n",
    "    return len(dups & labelled_dups) / LEN_LABELLED_DUPLICATES\n",
    "\n",
    "def _precision(row):\n",
    "    labelled_dups = set(row['duplicates'])\n",
    "    dups = set(row['predictions'])\n",
    "    LEN_DUPLICATES = len(dups)\n",
    "    if LEN_DUPLICATES == 0:\n",
    "        return 0\n",
    "    return len(dups & labelled_dups) / LEN_DUPLICATES\n",
    "\n",
    "with open(\"temp/uf.pkl\", \"rb\") as f:\n",
    "    uf = pickle.load(f)\n",
    "\n",
    "id2cluster = defaultdict(set)\n",
    "for id, cluster in uf.parent.items():\n",
    "    id2cluster[cluster].add(id)\n",
    "\n",
    "predictions = {id2core_id[x[\"id\"]]: set([id2core_id[neighbor] for neighbor in id2cluster[uf.find(x[\"id\"])] if neighbor != x[\"id\"]]) for x in truth}\n",
    "df = pd.Series(labels).to_frame(\"duplicates\").reset_index().merge(pd.Series(predictions).to_frame(\"predictions\").reset_index(), on=\"index\")\n",
    "\n",
    "df['Correct'] = df.apply(lambda row: set(row['duplicates']) == set(row['predictions']), axis=1).astype(int)\n",
    "prediction_summary = { 'Correct' : df['Correct'].sum(), 'Incorrect' : df.shape[0] - df['Correct'].sum() }\n",
    "prediction_summary['Accuracy'] = round(prediction_summary['Correct'] / df.shape[0], 4)\n",
    "\n",
    "recalls = df.apply(lambda row: _recall(row), axis=1)\n",
    "prediction_summary['Recall'] = round(recalls.mean(), 4)\n",
    "\n",
    "precisions = df.apply(lambda row: _precision(row), axis=1)\n",
    "prediction_summary['Precision'] = round(precisions.mean(), 4)\n",
    "\n",
    "prediction_summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "!python -m text_dedup.simhash --path ./temp_inp --local --column text --output temp_simhash --split train --debug \\\n",
    "    --bit_diff 6 \\\n",
    "    --num_bucket 7 \\\n",
    "    --ngram 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Correct': 81331,\n",
       " 'Incorrect': 18669,\n",
       " 'Accuracy': 0.8133,\n",
       " 'Recall': 0.8328,\n",
       " 'Precision': 0.347}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _recall(row):\n",
    "    labelled_dups = set(row['duplicates'])\n",
    "    LEN_LABELLED_DUPLICATES = len(labelled_dups)    \n",
    "    if LEN_LABELLED_DUPLICATES == 0:\n",
    "        return 1\n",
    "    dups = set(row['predictions'])\n",
    "    return len(dups & labelled_dups) / LEN_LABELLED_DUPLICATES\n",
    "\n",
    "def _precision(row):\n",
    "    labelled_dups = set(row['duplicates'])\n",
    "    dups = set(row['predictions'])\n",
    "    LEN_DUPLICATES = len(dups)\n",
    "    if LEN_DUPLICATES == 0:\n",
    "        return 0\n",
    "    return len(dups & labelled_dups) / LEN_DUPLICATES\n",
    "\n",
    "with open(\"temp_simhash/uf.pkl\", \"rb\") as f:\n",
    "    uf = pickle.load(f)\n",
    "\n",
    "id2cluster = defaultdict(set)\n",
    "for id, cluster in uf.parent.items():\n",
    "    id2cluster[cluster].add(id)\n",
    "\n",
    "predictions = {id2core_id[x[\"id\"]]: set([id2core_id[neighbor] for neighbor in id2cluster[uf.find(x[\"id\"])] if neighbor != x[\"id\"]]) for x in truth}\n",
    "df = pd.Series(labels).to_frame(\"duplicates\").reset_index().merge(pd.Series(predictions).to_frame(\"predictions\").reset_index(), on=\"index\")\n",
    "\n",
    "df['Correct'] = df.apply(lambda row: set(row['duplicates']) == set(row['predictions']), axis=1).astype(int)\n",
    "prediction_summary = { 'Correct' : df['Correct'].sum(), 'Incorrect' : df.shape[0] - df['Correct'].sum() }\n",
    "prediction_summary['Accuracy'] = round(prediction_summary['Correct'] / df.shape[0], 4)\n",
    "\n",
    "recalls = df.apply(lambda row: _recall(row), axis=1)\n",
    "prediction_summary['Recall'] = round(recalls.mean(), 4)\n",
    "\n",
    "precisions = df.apply(lambda row: _precision(row), axis=1)\n",
    "prediction_summary['Precision'] = round(precisions.mean(), 4)\n",
    "\n",
    "prediction_summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deduplication of Scholarly Documents using Locality Sensitive Hashing and Word Embeddings](https://aclanthology.org/2020.lrec-1.113) (Gyawali et al., LREC 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8994, Recall: 0.6783, F1: 0.7734\n",
      "Precision: 0.7681, Recall: 0.9335, F1: 0.8428\n",
      "Macro Average F1: 0.8081, Accuracy: 0.8133\n"
     ]
    }
   ],
   "source": [
    "def classify_in_paper(record):\n",
    "    duplicates = set(record['duplicates'])\n",
    "    predictions = set(record['predictions'])\n",
    "    \n",
    "    LEN_PREDICTIONS = len(predictions)\n",
    "    LEN_DUPLICATES = len(duplicates)\n",
    "\n",
    "    # if len(predictions) == 0 it is Negative whether True or not. \n",
    "    # Hopefully True is more common and short circuit ifs\n",
    "    if LEN_PREDICTIONS == 0:\n",
    "        if LEN_DUPLICATES == 0:\n",
    "            return 'TN'\n",
    "        if LEN_DUPLICATES > 0:\n",
    "            return 'FN'\n",
    "\n",
    "    # If len(predictions) > 0 it is Positive whether True or not.\n",
    "    # Hopefully True is more common and short circuit ifs\n",
    "    # python uses short circuiting so this is more readable and faster\n",
    "    if LEN_PREDICTIONS > 0:\n",
    "        if LEN_DUPLICATES > 0 and duplicates.issubset(predictions):\n",
    "            return 'TP'\n",
    "        if LEN_DUPLICATES == 0 or not duplicates.issubset(predictions):\n",
    "            return 'FP'\n",
    "    \n",
    "    raise ValueError(f'This should not happen {duplicates} {predictions} {len(duplicates)=} {len(predictions)=}')\n",
    "\n",
    "def inverse(label: str) ->str:\n",
    "    # inverts the results basically N->P and P->N\n",
    "    return {'TP': 'TN', 'FN': 'FP', 'FP': 'FN', 'TN': 'TP'}[label]\n",
    "\n",
    "df['Class'] = df.apply(lambda row: classify_in_paper(row), axis=1)\n",
    "df['Class_'] = df.apply(lambda row: inverse(row['Class']), axis=1)\n",
    "\n",
    "f1s = []\n",
    "for col in ['Class', 'Class_']:\n",
    "    label_counts = df[col].value_counts()\n",
    "    precision = label_counts['TP'] / (label_counts['TP'] + label_counts['FP'])\n",
    "    recall = label_counts['TP'] / (label_counts['TP'] + label_counts['FN'])\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
    "    f1s.append(f1)\n",
    "print(f'Macro Average F1: {sum(f1s) / len(f1s):.4f}, Accuracy: {df[\"Correct\"].mean():.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers seem too good to be true compared with what we see in the paper. Let's double check their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bbea2f588c4c4280765103d415f1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8302, Recall: 0.5521, F1: 0.6632\n",
      "Precision: 0.7098, Recall: 0.9065, F1: 0.7962\n",
      "Macro Average F1: 0.7297, Accuracy: 0.7456\n"
     ]
    }
   ],
   "source": [
    "title2core_ids = defaultdict(set)\n",
    "for record in ds:\n",
    "    title = record['processed_title']\n",
    "    core_id = int(record['core_id'])\n",
    "    title2core_ids[title].add(core_id)\n",
    "\n",
    "matches = ds.map(lambda row: {'matches': set(x for x in title2core_ids[row[\"processed_title\"]] if x != int(row[\"core_id\"]))})\n",
    "matches = {int(x[\"core_id\"]): x[\"matches\"] for x in matches}\n",
    "\n",
    "ddf = pd.Series(matches).to_frame(\"predictions\").reset_index().merge(df.drop(\"predictions\", axis=1), on=\"index\")\n",
    "ddf[\"Correct\"] = ddf.apply(lambda row: set(row['duplicates']) == set(row['predictions']), axis=1).astype(int)\n",
    "ddf['Class'] = ddf.apply(lambda row: classify_in_paper(row), axis=1)\n",
    "ddf['Class_'] = ddf.apply(lambda row: inverse(row['Class']), axis=1)\n",
    "\n",
    "f1s = []\n",
    "for col in ['Class', 'Class_']:\n",
    "    label_counts = ddf[col].value_counts()\n",
    "    precision = label_counts['TP'] / (label_counts['TP'] + label_counts['FP'])\n",
    "    recall = label_counts['TP'] / (label_counts['TP'] + label_counts['FN'])\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
    "    f1s.append(f1)\n",
    "print(f'Macro Average F1: {sum(f1s) / len(f1s):.4f}, Accuracy: {ddf[\"Correct\"].mean():.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is strange: precisions and accuracy are the same, but not the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't forget to cleanup cache files.\n",
    "ds.cleanup_cache_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "!rm -r temp_inp temp* ../temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-dedup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34b527130893b47044197df5fe15869983e660be4f2927608e2aeec6a74366e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
